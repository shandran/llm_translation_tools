{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3091323f-9b5c-4b67-a397-4a646d55e41c",
   "metadata": {},
   "source": [
    "**LLM Translation with Evaluator Proofreading (Markdown version)**    \n",
    "Using code created by Ed Donner in his Udemy Agentic AI course  \n",
    "*Translator: Gemini model*  \n",
    "*Evaluator: OpenAI model*  \n",
    "\n",
    "George MacDonald's [***The Princess and the Goblin***](https://github.com/mlschmitt/classic-books-markdown/blob/main/George%20MacDonald/The%20Princess%20and%20the%20Goblin.md) markdown file available from [**Michael Schmitt**](https://github.com/mlschmitt)'s [**classic-books-markdown**](https://github.com/mlschmitt/classic-books-markdown) repo on GitHub. \n",
    "\n",
    "For a brief overview and background, visit [**LLM Language Translation Tools**](https://github.com/shandran/llm_translation_tools/tree/main) page on my GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a992e9b2-79a5-48ce-87f9-243a2653b231",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ee7bed-3477-4520-ac75-2d1b1fa3a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "from docx import Document\n",
    "import re\n",
    "import tiktoken\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from openai import NotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f04c8b-3e6a-43fc-a9a3-1d24839a9a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load API keys\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50bcc87-4f0c-4a1f-91ae-7d70a1d6c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1abae4ec-a458-4a2c-b69a-a98fb6ed2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM model to use\n",
    "openai_model =  'gpt-4o' # 'o3-2025-04-16' # For better quality but higher cost: 'o3-2025-04-16' # 200K context window 100K max output tokens $2/$8\n",
    "gemini_model = 'gemini-2.5-flash' # gemini-2.5-pro' # 1M context window 65K max output tokens $1.25/$10 <200K $2.50/$15 >=200K tokens \n",
    "claude_model = 'claude-sonnet-4-0' # 200K context window 65K max output tokens $3/$15\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = Anthropic()\n",
    "gemini = genai.GenerativeModel(model_name=gemini_model)\n",
    "\n",
    "# define languages\n",
    "source_language = 'English'\n",
    "translated_language = 'Traditional Chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9670ff3-5288-4096-915a-07b8b6f71562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini model: gemini-2.5-pro\n",
      "OpenAI model: o3-2025-04-16\n",
      "Claude model: claude-sonnet-4-0\n",
      "Translation language: Traditional Chinese\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gemini model: {gemini_model}\")\n",
    "print(f\"OpenAI model: {openai_model}\")\n",
    "print(f\"Claude model: {claude_model}\")\n",
    "print(f\"Translation language: {translated_language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d53a135-d596-41a2-ac5c-1864ef811e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open md document with source material\n",
    "filename = \"George_MacDonald_The_Princess_and_the_Goblin_50_lines.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c74b9726-ffc4-4245-9af5-84e39d5899bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 (265 tokens) of 12 ---\n",
      "# Title: The Princess and the Goblin\n",
      "\n",
      "## Author: George MacDonald\n",
      "\n",
      "## Year: 1872\n",
      "\n",
      "-------\n",
      "\n",
      "###  CHAPTER 1 - Why the Princess Has a Story About Her\n",
      "\n",
      "There was once a little princess whose father was king over a great country full of mountains and valleys. His palace was built upon one of the mountain...\n",
      "\n",
      "\n",
      "--- Chunk 2 (126 tokens) of 12 ---\n",
      "These mountains were full of hollow places underneath; huge caverns, and winding ways, some with water running through them, and some shining with all colours of the rainbow when a light was taken in. There would not have been much known about them, had there not been mines there, great deep pits, w...\n",
      "\n",
      "\n",
      "--- Chunk 3 (624 tokens) of 12 ---\n",
      "Now in these subterranean caverns lived a strange race of beings, called by some gnomes, by some kobolds, by some goblins. There was a legend current in the country that at one time they lived above ground, and were very like other people. But for some reason or other, concerning which there were di...\n",
      "\n",
      "\n",
      "--- Chunk 4 (35 tokens) of 12 ---\n",
      "###  CHAPTER 2 - The Princess Loses Herself\n",
      "\n",
      "I have said the Princess Irene was about eight years old when my story begins. And this is how it begins.\n",
      "\n",
      "\n",
      "--- Chunk 5 (332 tokens) of 12 ---\n",
      "One very wet day, when the mountain was covered with mist which was constantly gathering itself together into raindrops, and pouring down on the roofs of the great old house, whence it fell in a fringe of water from the eaves all round about it, the princess could not of course go out. She got very ...\n",
      "\n",
      "\n",
      "--- Chunk 6 (106 tokens) of 12 ---\n",
      "Even that is a change, and the princess wakes up a little, and looks about her. Then she tumbles off her chair and runs out of the door, not the same door the nurse went out of, but one which opened at the foot of a curious old stair of worm-eaten oak, which looked as if never anyone had set foot up...\n",
      "\n",
      "\n",
      "--- Chunk 7 (199 tokens) of 12 ---\n",
      "Up and up she ran--such a long way it seemed to her!--until she came to the top of the third flight. There she found the landing was the end of a long passage. Into this she ran. It was full of doors on each side. There were so many that she did not care to open any, but ran on to the end, where she...\n",
      "\n",
      "\n",
      "--- Chunk 8 (108 tokens) of 12 ---\n",
      "She ran for some distance, turned several times, and then began to be afraid. Very soon she was sure that she had lost the way back. Rooms everywhere, and no stair! Her little heart beat as fast as her little feet ran, and a lump of tears was growing in her throat. But she was too eager and perhaps ...\n",
      "\n",
      "\n",
      "--- Chunk 9 (250 tokens) of 12 ---\n",
      "She did not cry long, however, for she was as brave as could be expected of a princess of her age. After a good cry, she got up, and brushed the dust from her frock. Oh, what old dust it was! Then she wiped her eyes with her hands, for princesses don't always have their handkerchiefs in their pocket...\n",
      "\n",
      "\n",
      "--- Chunk 10 (258 tokens) of 12 ---\n",
      "When she came to the top, she found herself in a little square place, with three doors, two opposite each other, and one opposite the top of the stair. She stood for a moment, without an idea in her little head what to do next. But as she stood, she began to hear a curious humming sound. Could it be...\n",
      "\n",
      "\n",
      "--- Chunk 11 (292 tokens) of 12 ---\n",
      "Perhaps you will wonder how the princess could tell that the old lady was an old lady, when I inform you that not only was she beautiful, but her skin was smooth and white. I will tell you more. Her hair was combed back from her forehead and face, and hung loose far down and all over her back. That ...\n",
      "\n",
      "\n",
      "--- Chunk 12 (64 tokens) of 12 ---\n",
      "'Come to me, my dear,' said the old lady.\n",
      "\n",
      "And again the princess did as she was told. She approached the old lady--rather slowly, I confess--but did not stop until she stood by her side, and looked up in her face with her blue eyes and the two melted stars in them.\n",
      "\n",
      "\n",
      "âœ… 12 chunks created and saved to 'markdown_chunks.json'\n"
     ]
    }
   ],
   "source": [
    "# Load the markdown file\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Set up tokenizer (GPT-4-compatible)\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")  # or \"gpt-4o\" if preferred\n",
    "\n",
    "# ---------- Structure Detection Helpers ----------\n",
    "def is_heading(line):\n",
    "    return re.match(r\"^#{1,6} \", line)\n",
    "\n",
    "def is_list_item(line):\n",
    "    return re.match(r\"^(\\s*[-*+] |\\s*\\d+\\.\\s)\", line)\n",
    "\n",
    "def is_blockquote(line):\n",
    "    return line.strip().startswith(\">\")\n",
    "\n",
    "def is_code_fence(line):\n",
    "    # This regex looks for lines that start with at least three backticks\n",
    "    # and optionally include language specifiers. It also handles closing fences.\n",
    "    return re.match(r\"^\\s*`{3,}\", line)\n",
    "\n",
    "def is_blank(line):\n",
    "    return line.strip() == \"\"\n",
    "\n",
    "# ---------- Group Lines into Logical Blocks ----------\n",
    "blocks = []\n",
    "current_block = []\n",
    "in_code_block = False\n",
    "in_blockquote_block = False # New flag for blockquote\n",
    "\n",
    "for line in lines:\n",
    "    if is_code_fence(line):\n",
    "        # If exiting a blockquote before a code fence, close it out\n",
    "        if in_blockquote_block and current_block:\n",
    "            blocks.append(\"\".join(current_block).strip())\n",
    "            current_block = []\n",
    "            in_blockquote_block = False\n",
    "\n",
    "        in_code_block = not in_code_block\n",
    "        current_block.append(line)\n",
    "        if not in_code_block: # If just closed a code block\n",
    "            blocks.append(\"\".join(current_block).strip())\n",
    "            current_block = []\n",
    "        continue # Move to next line, as code fence handled itself\n",
    "\n",
    "    if in_code_block: # If currently inside a code block\n",
    "        current_block.append(line)\n",
    "        continue\n",
    "\n",
    "    # Handle blockquotes\n",
    "    if is_blockquote(line):\n",
    "        if not in_blockquote_block: # If starting a new blockquote block\n",
    "            if current_block: # If there's a preceding block of different type\n",
    "                blocks.append(\"\".join(current_block).strip())\n",
    "                current_block = []\n",
    "            in_blockquote_block = True\n",
    "        current_block.append(line) # Always append blockquote lines to current_block\n",
    "    else: # Not a blockquote line\n",
    "        if in_blockquote_block: # If exiting a blockquote block\n",
    "            if current_block: # Ensure there's content to append\n",
    "                blocks.append(\"\".join(current_block).strip())\n",
    "                current_block = []\n",
    "            in_blockquote_block = False # Reset the flag\n",
    "\n",
    "        # Handle other block types outside of blockquote logic\n",
    "        if is_heading(line) or is_list_item(line):\n",
    "            if current_block: # If there's a preceding block of different type\n",
    "                blocks.append(\"\".join(current_block).strip())\n",
    "                current_block = []\n",
    "            current_block.append(line)\n",
    "        elif is_blank(line):\n",
    "            if current_block: # If there's content to append before a blank line\n",
    "                blocks.append(\"\".join(current_block).strip())\n",
    "                current_block = []\n",
    "            # Do not append blank lines themselves as blocks, they act as separators\n",
    "        else: # Regular paragraph text\n",
    "            current_block.append(line)\n",
    "\n",
    "# Add any remaining content in current_block as a final block\n",
    "if current_block:\n",
    "    blocks.append(\"\".join(current_block).strip())\n",
    "\n",
    "# ---------- Group Blocks into Token-Aware Chunks ----------\n",
    "target_tokens = 300 # 150\n",
    "max_tokens = 400 # 180 # Not explicitly used for dynamic splitting in this structure, but good for upper bound\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "current_token_count = 0\n",
    "chunk_id = 1\n",
    "\n",
    "for block in blocks:\n",
    "    block_tokens = len(enc.encode(block))\n",
    "\n",
    "    # If adding the current block exceeds the target and there's already content in the current chunk\n",
    "    # then finalize the current chunk and start a new one.\n",
    "    if current_token_count + block_tokens > target_tokens and current_chunk:\n",
    "        chunk_text = \"\\n\\n\".join(current_chunk) # This is where blocks are joined\n",
    "        chunks.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"tokens\": current_token_count,\n",
    "            \"text\": chunk_text\n",
    "        })\n",
    "        chunk_id += 1\n",
    "        current_chunk = []\n",
    "        current_token_count = 0\n",
    "\n",
    "    current_chunk.append(block)\n",
    "    current_token_count += block_tokens\n",
    "\n",
    "# Add final chunk if there's any remaining content\n",
    "if current_chunk:\n",
    "    chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "    chunks.append({\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"tokens\": current_token_count,\n",
    "        \"text\": chunk_text\n",
    "    })\n",
    "\n",
    "# ---------- Optional: Save as JSON ----------\n",
    "output_json_filename = \"markdown_chunks.json\"\n",
    "with open(output_json_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ---------- Preview ----------\n",
    "for chunk in chunks:\n",
    "    # Print only the first 300 characters for preview to avoid large output\n",
    "    print(f\"\\n--- Chunk {chunk['chunk_id']} ({chunk['tokens']} tokens) of {len(chunks)} ---\\n{chunk['text'][:300]}{'...' if len(chunk['text']) > 300 else ''}\\n\")\n",
    "print(f\"\\nâœ… {len(chunks)} chunks created and saved to '{output_json_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7531e74-b2f4-42f2-93e3-16866bf5c0d7",
   "metadata": {},
   "source": [
    "### Translation prompt (system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dcfdb2-f4bc-4722-86f6-6aabb1f43c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt instruction\n",
    "system_message = f\"\"\"\n",
    "You are a professional translator of classic literary works in English.\n",
    "Your task is to translate chunked markdown inputs (headings, paragraphs, blockquotes, lists, code blocks, etc.) from {source_language} into {translated_language}, preserving all original formatting and markdown syntax exactly.\n",
    "\n",
    "Requirements:\n",
    "1. Faithfully render every word and phrase; do NOT summarize, simplify, omit, or add commentary.\n",
    "2. Use vocabulary and idioms familiar to a general readership audience.\n",
    "3. Preserve all punctuation, inline code, and block structures.\n",
    "4. Maintain consistent translation approach across all chunks.\n",
    "5. Retain capitalization of proper names and headings exactly as in the source.\n",
    "6. Output only the translated markdown text: no explanations, no extra whitespace beyond what is in the input.\n",
    "\n",
    "Start each response with the translated chunk; do not include any metadata or system commentary.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": system_message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a8c7c1-a9e8-4867-81a5-45011e91f221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a professional translator of classic literary works in English.\\nYour task is to translate chunked markdown inputs (headings, paragraphs, blockquotes, lists, code blocks, etc.) from English into Traditional Chinese, preserving all original formatting and markdown syntax exactly.\\n\\nRequirements:\\n1. Faithfully render every word and phrase; do NOT summarize, simplify, omit, or add commentary.\\n2. Use vocabulary and idioms familiar to a general readership audience.\\n3. Preserve all punctuation, inline code, and block structures.\\n4. Maintain consistent translation approach across all chunks.\\n5. Retain capitalization of proper names and headings exactly as in the source.\\n6. Output only the translated markdown text: no explanations, no extra whitespace beyond what is in the input.\\n\\nStart each response with the translated chunk; do not include any metadata or system commentary.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed8877f7-04cf-4552-967c-d6867d8b44e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-2.5-pro',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706b908f-ffbd-49a5-86f7-c20c22491def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini model: gemini-2.5-pro\n",
      "OpenAI model: o3-2025-04-16\n",
      "Claude model: claude-sonnet-4-0\n",
      "Translation language: Traditional Chinese\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gemini model: {gemini_model}\")\n",
    "print(f\"OpenAI model: {openai_model}\")\n",
    "print(f\"Claude model: {claude_model}\")\n",
    "print(f\"Translation language: {translated_language}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639fa2c4-eb53-4157-b3c8-47519abb496c",
   "metadata": {},
   "source": [
    "### Translation function (translate_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475888ef-2294-488e-bff8-1af8152f814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translator function with error reporting to 'error', handling empty returns, and a settable max retries \n",
    "\n",
    "def translate_chunk(\n",
    "    text: str,\n",
    "    system_message: str,\n",
    "    idx: int = None,\n",
    "    total: int = None,\n",
    "    retry_on_rate_limit: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Sends one chunk of text through Gemini and returns the translated string.\n",
    "    Retries once on exceptions or empty responses, and records errors in chunk['error'].\n",
    "    If both the initial call and retry fail, returns the sentinel \"translation failed\".\n",
    "    \"\"\"\n",
    "    if idx is not None and total is not None:\n",
    "        print(\n",
    "            f\"Translating chunk {idx}/{total} \"\n",
    "            f\"(first 50 chars): {text[:50]}... | Type: {type(text)}\"\n",
    "        )\n",
    "   \n",
    "    prompt = system_message + \"\\n\\n\" + text\n",
    "    attempt = 0\n",
    "    MAX_RETRIES = 1\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = gemini.generate_content(\n",
    "                contents=[{\"role\": \"user\", \"parts\": [prompt]}]\n",
    "            )\n",
    "            out = response.text.strip()\n",
    "            if not out:\n",
    "                # treat empty response as an error\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            return out\n",
    "\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            print(f\"âš ï¸  Translation error on chunk {idx}: {e}\")\n",
    "            # record the error into the chunk dict if available\n",
    "            try:\n",
    "                chunk.setdefault('error', str(e))\n",
    "            except NameError:\n",
    "                pass  # chunk not in scope here\n",
    "\n",
    "            if not retry_on_rate_limit or attempt > MAX_RETRIES:\n",
    "                print(f\"â€¼ï¸  Translation failed for chunk {idx}, moving on.\")\n",
    "                return \"translation failed\"\n",
    "\n",
    "            print(\"â€¦retrying after brief pauseâ€¦\")\n",
    "            time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82cb085-2110-4ac5-b925-db25066bcc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffe2189d-334b-4e1b-9f45-f17a19005878",
   "metadata": {},
   "source": [
    "### Call translation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f2c1b7-ee2b-408f-abec-e67bacbabf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/12 (first 50 chars): # Title: The Princess and the Goblin\n",
      "\n",
      "## Author: G... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 1\n",
      "\n",
      "Translating chunk 2/12 (first 50 chars): These mountains were full of hollow places underne... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 2\n",
      "\n",
      "Translating chunk 3/12 (first 50 chars): Now in these subterranean caverns lived a strange ... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 3\n",
      "\n",
      "Translating chunk 4/12 (first 50 chars): ###  CHAPTER 2 - The Princess Loses Herself\n",
      "\n",
      "I hav... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 4\n",
      "\n",
      "Translating chunk 5/12 (first 50 chars): One very wet day, when the mountain was covered wi... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 5\n",
      "\n",
      "Translating chunk 6/12 (first 50 chars): Even that is a change, and the princess wakes up a... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 6\n",
      "\n",
      "Translating chunk 7/12 (first 50 chars): Up and up she ran--such a long way it seemed to he... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 7\n",
      "\n",
      "Translating chunk 8/12 (first 50 chars): She ran for some distance, turned several times, a... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 8\n",
      "\n",
      "Translating chunk 9/12 (first 50 chars): She did not cry long, however, for she was as brav... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 9\n",
      "\n",
      "Translating chunk 10/12 (first 50 chars): When she came to the top, she found herself in a l... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 10\n",
      "\n",
      "Translating chunk 11/12 (first 50 chars): Perhaps you will wonder how the princess could tel... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 11\n",
      "\n",
      "Translating chunk 12/12 (first 50 chars): 'Come to me, my dear,' said the old lady.\n",
      "\n",
      "And aga... | Type: <class 'str'>\n",
      "âœ… Updated chunk_id 12\n",
      "\n",
      "â±ï¸ Total processing time: 0h 5m 38.27s\n"
     ]
    }
   ],
   "source": [
    "# Start the master timer\n",
    "master_start = time.perf_counter()\n",
    "\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    chunk['error'] = None\n",
    "    translated = translate_chunk(\n",
    "        chunk['text'],\n",
    "        system_message,\n",
    "        idx=i,\n",
    "        total=len(chunks),\n",
    "        retry_on_rate_limit=True\n",
    "    )\n",
    "    chunk['check'] = translated\n",
    "    print(f\"âœ… Updated chunk_id {chunk['chunk_id']}\\n\")\n",
    "\n",
    "# Stop the master timer and report H:M:S\n",
    "master_end = time.perf_counter()\n",
    "total_seconds = master_end - master_start\n",
    "hours   = int(total_seconds // 3600)\n",
    "minutes = int((total_seconds % 3600) // 60)\n",
    "seconds = total_seconds % 60\n",
    "\n",
    "print(\n",
    "    f\"â±ï¸ Total processing time: \"\n",
    "    f\"{hours}h {minutes}m {seconds:.2f}s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8772f70e-556c-40a0-81c5-a791660163d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82505508-b529-453f-8569-26a0dfee0f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_id': 1,\n",
       " 'tokens': 265,\n",
       " 'text': '# Title: The Princess and the Goblin\\n\\n## Author: George MacDonald\\n\\n## Year: 1872\\n\\n-------\\n\\n###  CHAPTER 1 - Why the Princess Has a Story About Her\\n\\nThere was once a little princess whose father was king over a great country full of mountains and valleys. His palace was built upon one of the mountains, and was very grand and beautiful. The princess, whose name was Irene, was born there, but she was sent soon after her birth, because her mother was not very strong, to be brought up by country people in a large house, half castle, half farmhouse, on the side of another mountain, about half-way between its base and its peak.\\n\\nThe princess was a sweet little creature, and at the time my story begins was about eight years old, I think, but she got older very fast. Her face was fair and pretty, with eyes like two bits of night sky, each with a star dissolved in the blue. Those eyes you would have thought must have known they came from there, so often were they turned up in that direction. The ceiling of her nursery was blue, with stars in it, as like the sky as they could make it. But I doubt if ever she saw the real sky with the stars in it, for a reason which I had better mention at once.',\n",
       " 'error': None,\n",
       " 'check': '# æ›¸åï¼šå…¬ä¸»èˆ‡å“¥å¸ƒæ—\\n\\n## ä½œè€…ï¼šå–¬æ²»Â·éº¥å…‹å”ç´\\n\\n## å¹´ä»½ï¼š1872\\n\\n-------\\n\\n###  ç¬¬ä¸€ç«  - ç‚ºä½•å…¬ä¸»è¦æœ‰å€‹æ•…äº‹\\n\\nå¾å‰ï¼Œæœ‰ä½å°å…¬ä¸»ï¼Œå¥¹çš„çˆ¶ç‹çµ±æ²»è‘—ä¸€å€‹æ»¿æ˜¯ç¾¤å±±èˆ‡è°·åœ°çš„å¤§åœ‹ã€‚ä»–çš„å®®æ®¿å»ºåœ¨ä¸€åº§å±±ä¸Šï¼Œæ—¢å®å‰åˆç¾éº—ã€‚é€™ä½åå«Ireneçš„å…¬ä¸»å°±åœ¨é‚£è£¡å‡ºç”Ÿï¼Œä½†å¥¹å‡ºç”Ÿå¾Œä¸ä¹…ï¼Œå› ç‚ºæ¯è¦ªèº«é«”ä¸å¤ªå¥½ï¼Œä¾¿è¢«é€å»çµ¦é„‰ä¸‹äººæ’«é¤Šï¼Œä½åœ¨ä¸€æ£Ÿå¤§æˆ¿å­è£¡ï¼Œé‚£æˆ¿å­åŠæ˜¯åŸå ¡ï¼ŒåŠæ˜¯è¾²èŠï¼Œä½æ–¼å¦ä¸€åº§å±±çš„å±±è…°ä¸Šï¼Œå¤§ç´„åœ¨å±±è…³èˆ‡å±±é ‚ä¹‹é–“çš„ä¸­é»ã€‚\\n\\nå…¬ä¸»æ˜¯å€‹å¯æ„›çš„å°äººå…’ï¼Œåœ¨æˆ‘çš„æ•…äº‹é–‹å§‹æ™‚ï¼Œæˆ‘æƒ³ï¼Œå¥¹å¤§ç´„å…«æ­²ï¼Œä½†å¥¹é•·å¾—å¾ˆå¿«ã€‚å¥¹çš„è‡‰é¾ç™½çš™è€Œç¾éº—ï¼Œæœ‰è‘—ä¸€é›™åƒå…©ç‰‡å¤œç©ºçš„çœ¼ç›ï¼Œæ¯ä¸€ç‰‡æ¹›è—ä¸­éƒ½æº¶è‘—ä¸€é¡†æ˜Ÿæ˜Ÿã€‚ä½ æœƒä»¥ç‚ºé‚£é›™çœ¼ç›ä¸€å®šçŸ¥é“è‡ªå·±æ˜¯å¾é‚£è£¡ä¾†çš„ï¼Œå› ç‚ºå®ƒå€‘ç¸½æ˜¯é‚£éº¼é »ç¹åœ°æœé‚£å€‹æ–¹å‘ä»°æœ›ã€‚å¥¹è‚²å…’å®¤çš„å¤©èŠ±æ¿æ˜¯è—è‰²çš„ï¼Œä¸Šé¢æœ‰æ˜Ÿæ˜Ÿï¼Œä»–å€‘ç›¡å…¶æ‰€èƒ½åœ°å°‡å®ƒåšå¾—åƒå¤©ç©ºä¸€æ¨£ã€‚ä½†æˆ‘æ‡·ç–‘å¥¹æ˜¯å¦æ›¾è¦‹éçœŸæ­£æœ‰æ˜Ÿæ˜Ÿçš„å¤©ç©ºï¼ŒåŸå› æˆ‘æœ€å¥½ç¾åœ¨å°±èªªæ˜ç™½ã€‚'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad608416-0c9c-493e-93ce-0866454cdd05",
   "metadata": {},
   "source": [
    "#### Check for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "384093dc-a85b-420b-a485-f1b657c92951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No error: 12, With error: 0\n",
      "Chunks containing errors:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Identify and display error entries\n",
    "errors = [chunk for chunk in chunks if chunk.get('error')]\n",
    "\n",
    "# Count\n",
    "eno_error = len([c for c in chunks if c.get('error') is None])\n",
    "has_error = len(errors)\n",
    "print(f\"No error: {eno_error}, With error: {has_error}\")\n",
    "\n",
    "# Pretty-print error chunks\n",
    "print(\"Chunks containing errors:\")\n",
    "print(json.dumps(errors, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdbba164-1948-4ab5-8774-3096b4d7f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c57cf-0854-45e8-bcca-a77d3d9e2841",
   "metadata": {},
   "source": [
    "### Evaluation function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d806377b-1573-4caf-8bb9-1da6081418ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chunk(\n",
    "    chunk: dict,\n",
    "    system_message: str,\n",
    "    source_language: str,\n",
    "    translated_language: str,\n",
    "    translate_func,\n",
    "    idx: int = None,\n",
    "    total: int = None,\n",
    "    retry_on_failure: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    1) Runs primary evaluation on chunk['check'] and records chunk['primary_feedback'].\n",
    "    2) If passed, sets chunk['final']=chunk['check'], chunk['final_model']=gemini_model.\n",
    "    3) If failed and retry_on_failure, translates with Claude, records chunk['alt_feedback'], re-evaluates once.\n",
    "    4) On fallback pass, sets chunk['final']=fallback text and chunk['final_model']=claude_model; if both fail, final_model=None.\n",
    "    Returns chunk['final'].\n",
    "    \"\"\"\n",
    "\n",
    "    def _call_evaluator(text_to_check: str):\n",
    "        # Build the evaluation prompt with localized name exceptions\n",
    "        evaluation_prompt = f\"\"\"\n",
    "You are a professional translation evaluator reviewing translated classic literary works for a general audience. \n",
    "Your task is to assess whether a translated markdown chunk faithfully and accurately follows the original, based on the following criteria:\n",
    "\n",
    "The input text is chunked and therefore may include partial or incomplete sentences. Do not reject the accuracy of the translation due to chunking issues.\n",
    "\n",
    "1. **Accuracy**: The meaning is faithfully preserved â€” no additions, omissions, or summarizations.\n",
    "2. **Contextual Fidelity**: Terms are rendered precisely and consistently.\n",
    "3. **Markdown Integrity**: All formatting (headings, lists, blockquotes, punctuation, verse references, etc.) is preserved exactly.\n",
    "4. **Audience Awareness**: Language attempts to translate idioms and style as best as feasible for a translated work.\n",
    "5. **Proper Names and Capitalization**: All names and headings match the source formatting, or uses standard, established equivalents in target lan\n",
    "\n",
    "Source:\n",
    "{chunk['text']}\n",
    "\n",
    "Translation:\n",
    "{text_to_check}\n",
    "\n",
    "Respond with ONLY raw JSON:\n",
    "{{\n",
    "  \"passed\": true or false,\n",
    "  \"feedback\": \"If failed, brief specific reason; empty if passed\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=openai_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": evaluation_prompt}]\n",
    "        ).choices[0].message.content.strip()\n",
    "        clean = re.sub(r'^```(?:json)?\\r?\\n', '', resp)\n",
    "        clean = re.sub(r'\\r?\\n```$', '', clean)\n",
    "        return json.loads(clean)\n",
    "\n",
    "    # Primary evaluation\n",
    "    if idx and total:\n",
    "        print(f\"Evaluating chunk {idx}/{total} with {gemini_model}â€¦\", end=\" \")\n",
    "    verdict = _call_evaluator(chunk['check'])\n",
    "    chunk['primary_feedback'] = verdict.get('feedback', \"\")\n",
    "    chunk['passed'] = verdict.get('passed', False)\n",
    "\n",
    "    if chunk['passed']:\n",
    "        chunk['final'] = chunk['check']\n",
    "        chunk['final_model'] = gemini_model\n",
    "        print(\"â†’ primary passed\")\n",
    "        return chunk['final']\n",
    "\n",
    "    # Fallback to Claude\n",
    "    chunk['alt_feedback'] = None\n",
    "    if retry_on_failure:\n",
    "        print(\"â†’ primary failed; translating with Claudeâ€¦\")\n",
    "        claude_resp = claude.messages.create(\n",
    "                    model=claude_model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\",   \"content\": system_message + \"\\n\\n\" + chunk[\"text\"]\n",
    "                        }\n",
    "                    ],\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "        \n",
    "        alt_text = claude_resp.content[0].text\n",
    "        chunk['check_alt'] = alt_text\n",
    "\n",
    "        if idx and total:\n",
    "            print(f\"Re-evaluating chunk {idx}/{total} with {claude_model}â€¦\", end=\" \")\n",
    "        verdict2 = _call_evaluator(alt_text)\n",
    "        chunk['alt_feedback'] = verdict2.get('feedback', \"\")\n",
    "        chunk['passed'] = verdict2.get('passed', False)\n",
    "\n",
    "        if chunk['passed']:\n",
    "            chunk['final'] = alt_text\n",
    "            chunk['final_model'] = claude_model\n",
    "            print(\"â†’ fallback passed\")\n",
    "            return chunk['final']\n",
    "\n",
    "    # Both failed\n",
    "    reason = chunk['alt_feedback'] or chunk['primary_feedback'] or 'Unknown reason'\n",
    "    chunk['final'] = (\n",
    "        f\"Translation into {translated_language} failed: {reason}\"\n",
    "    )\n",
    "    chunk['final_model'] = None\n",
    "    print(f\"â†’ both evaluations failed: {reason}\")\n",
    "    return chunk['final']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f697660-8512-44a1-b9c8-2e06d2c0f919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05b29e37-5417-413a-9ad9-58356bfbc65e",
   "metadata": {},
   "source": [
    "### Call the Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e227307-9849-4cfd-a9ba-0bab45af2294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1/12 (ID: 1) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 1/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 1 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 2/12 (ID: 2) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 2/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 2 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 3/12 (ID: 3) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 3/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 3 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 4/12 (ID: 4) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 4/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 4 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 5/12 (ID: 5) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 5/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 5 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 6/12 (ID: 6) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 6/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 6 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 7/12 (ID: 7) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 7/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 7 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 8/12 (ID: 8) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 8/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 8 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 9/12 (ID: 9) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 9/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 9 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 10/12 (ID: 10) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 10/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 10 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 11/12 (ID: 11) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 11/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 11 passed! Saved to chunk['final'].\n",
      "\n",
      "\n",
      "--- Chunk 12/12 (ID: 12) ---\n",
      "âœ… Using existing translation; starting evaluationâ€¦\n",
      "Evaluating chunk 12/12 with gemini-2.5-proâ€¦ â†’ primary passed\n",
      "ğŸ‰ Chunk 12 passed! Saved to chunk['final'].\n",
      "\n",
      "â±ï¸ Total evaluation time: 0h 3m 33.40s\n"
     ]
    }
   ],
   "source": [
    "# 1) Start the overall timer\n",
    "master_start = time.perf_counter()\n",
    "\n",
    "for idx, chunk in enumerate(chunks, start=1):\n",
    "    # Skip chunks already evaluated\n",
    "    if 'passed' in chunk and 'final_model' in chunk:\n",
    "        print(f\"--- Chunk {idx}/{len(chunks)} (ID: {chunk['chunk_id']}) already evaluated; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Initialize evaluator-stage error field only\n",
    "    chunk['eval_error'] = None\n",
    "\n",
    "    print(f\"\\n--- Chunk {idx}/{len(chunks)} (ID: {chunk['chunk_id']}) ---\")\n",
    "    print(\"âœ… Using existing translation; starting evaluationâ€¦\")\n",
    "\n",
    "    try:\n",
    "        # Call the unified evaluate_chunk (includes fallback)\n",
    "        final_text = evaluate_chunk(\n",
    "            chunk,\n",
    "            system_message,\n",
    "            source_language,\n",
    "            translated_language,\n",
    "            translate_chunk,\n",
    "            idx=idx,\n",
    "            total=len(chunks),\n",
    "            retry_on_failure=True\n",
    "        )\n",
    "        chunk['final'] = final_text\n",
    "    except Exception as e:\n",
    "        # Catch any unexpected errors in evaluation\n",
    "        chunk['eval_error'] = str(e)\n",
    "        chunk['final'] = f\"Evaluation error: {e}\"\n",
    "        chunk['final_model'] = None\n",
    "        print(f\"âš ï¸  Unexpected evaluation error on chunk {chunk['chunk_id']}: {e}\")\n",
    "\n",
    "    # Report outcome\n",
    "    if chunk['final'].startswith(\"Translation into\") or chunk['final'].startswith(\"Evaluation error\"):\n",
    "        print(f\"â€¼ï¸  Chunk {chunk['chunk_id']} ultimately failed.\\n\")\n",
    "    else:\n",
    "        print(f\"ğŸ‰ Chunk {chunk['chunk_id']} passed! Saved to chunk['final'].\\n\")\n",
    "\n",
    "# 2) Stop the overall timer\n",
    "master_end = time.perf_counter()\n",
    "total_seconds = master_end - master_start\n",
    "hours = int(total_seconds // 3600)\n",
    "minutes = int((total_seconds % 3600) // 60)\n",
    "seconds = total_seconds % 60\n",
    "\n",
    "print(\n",
    "    f\"â±ï¸ Total evaluation time: {hours}h {minutes}m {seconds:.2f}s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0310e0-8906-4914-b252-e93e9025f9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dc27452-5ffa-4199-aff9-196372e0f2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_id': 1,\n",
       " 'tokens': 265,\n",
       " 'text': '# Title: The Princess and the Goblin\\n\\n## Author: George MacDonald\\n\\n## Year: 1872\\n\\n-------\\n\\n###  CHAPTER 1 - Why the Princess Has a Story About Her\\n\\nThere was once a little princess whose father was king over a great country full of mountains and valleys. His palace was built upon one of the mountains, and was very grand and beautiful. The princess, whose name was Irene, was born there, but she was sent soon after her birth, because her mother was not very strong, to be brought up by country people in a large house, half castle, half farmhouse, on the side of another mountain, about half-way between its base and its peak.\\n\\nThe princess was a sweet little creature, and at the time my story begins was about eight years old, I think, but she got older very fast. Her face was fair and pretty, with eyes like two bits of night sky, each with a star dissolved in the blue. Those eyes you would have thought must have known they came from there, so often were they turned up in that direction. The ceiling of her nursery was blue, with stars in it, as like the sky as they could make it. But I doubt if ever she saw the real sky with the stars in it, for a reason which I had better mention at once.',\n",
       " 'error': None,\n",
       " 'check': '# æ›¸åï¼šå…¬ä¸»èˆ‡å“¥å¸ƒæ—\\n\\n## ä½œè€…ï¼šå–¬æ²»Â·éº¥å…‹å”ç´\\n\\n## å¹´ä»½ï¼š1872\\n\\n-------\\n\\n###  ç¬¬ä¸€ç«  - ç‚ºä½•å…¬ä¸»è¦æœ‰å€‹æ•…äº‹\\n\\nå¾å‰ï¼Œæœ‰ä½å°å…¬ä¸»ï¼Œå¥¹çš„çˆ¶ç‹çµ±æ²»è‘—ä¸€å€‹æ»¿æ˜¯ç¾¤å±±èˆ‡è°·åœ°çš„å¤§åœ‹ã€‚ä»–çš„å®®æ®¿å»ºåœ¨ä¸€åº§å±±ä¸Šï¼Œæ—¢å®å‰åˆç¾éº—ã€‚é€™ä½åå«Ireneçš„å…¬ä¸»å°±åœ¨é‚£è£¡å‡ºç”Ÿï¼Œä½†å¥¹å‡ºç”Ÿå¾Œä¸ä¹…ï¼Œå› ç‚ºæ¯è¦ªèº«é«”ä¸å¤ªå¥½ï¼Œä¾¿è¢«é€å»çµ¦é„‰ä¸‹äººæ’«é¤Šï¼Œä½åœ¨ä¸€æ£Ÿå¤§æˆ¿å­è£¡ï¼Œé‚£æˆ¿å­åŠæ˜¯åŸå ¡ï¼ŒåŠæ˜¯è¾²èŠï¼Œä½æ–¼å¦ä¸€åº§å±±çš„å±±è…°ä¸Šï¼Œå¤§ç´„åœ¨å±±è…³èˆ‡å±±é ‚ä¹‹é–“çš„ä¸­é»ã€‚\\n\\nå…¬ä¸»æ˜¯å€‹å¯æ„›çš„å°äººå…’ï¼Œåœ¨æˆ‘çš„æ•…äº‹é–‹å§‹æ™‚ï¼Œæˆ‘æƒ³ï¼Œå¥¹å¤§ç´„å…«æ­²ï¼Œä½†å¥¹é•·å¾—å¾ˆå¿«ã€‚å¥¹çš„è‡‰é¾ç™½çš™è€Œç¾éº—ï¼Œæœ‰è‘—ä¸€é›™åƒå…©ç‰‡å¤œç©ºçš„çœ¼ç›ï¼Œæ¯ä¸€ç‰‡æ¹›è—ä¸­éƒ½æº¶è‘—ä¸€é¡†æ˜Ÿæ˜Ÿã€‚ä½ æœƒä»¥ç‚ºé‚£é›™çœ¼ç›ä¸€å®šçŸ¥é“è‡ªå·±æ˜¯å¾é‚£è£¡ä¾†çš„ï¼Œå› ç‚ºå®ƒå€‘ç¸½æ˜¯é‚£éº¼é »ç¹åœ°æœé‚£å€‹æ–¹å‘ä»°æœ›ã€‚å¥¹è‚²å…’å®¤çš„å¤©èŠ±æ¿æ˜¯è—è‰²çš„ï¼Œä¸Šé¢æœ‰æ˜Ÿæ˜Ÿï¼Œä»–å€‘ç›¡å…¶æ‰€èƒ½åœ°å°‡å®ƒåšå¾—åƒå¤©ç©ºä¸€æ¨£ã€‚ä½†æˆ‘æ‡·ç–‘å¥¹æ˜¯å¦æ›¾è¦‹éçœŸæ­£æœ‰æ˜Ÿæ˜Ÿçš„å¤©ç©ºï¼ŒåŸå› æˆ‘æœ€å¥½ç¾åœ¨å°±èªªæ˜ç™½ã€‚',\n",
       " 'eval_error': None,\n",
       " 'primary_feedback': '',\n",
       " 'passed': True,\n",
       " 'final': '# æ›¸åï¼šå…¬ä¸»èˆ‡å“¥å¸ƒæ—\\n\\n## ä½œè€…ï¼šå–¬æ²»Â·éº¥å…‹å”ç´\\n\\n## å¹´ä»½ï¼š1872\\n\\n-------\\n\\n###  ç¬¬ä¸€ç«  - ç‚ºä½•å…¬ä¸»è¦æœ‰å€‹æ•…äº‹\\n\\nå¾å‰ï¼Œæœ‰ä½å°å…¬ä¸»ï¼Œå¥¹çš„çˆ¶ç‹çµ±æ²»è‘—ä¸€å€‹æ»¿æ˜¯ç¾¤å±±èˆ‡è°·åœ°çš„å¤§åœ‹ã€‚ä»–çš„å®®æ®¿å»ºåœ¨ä¸€åº§å±±ä¸Šï¼Œæ—¢å®å‰åˆç¾éº—ã€‚é€™ä½åå«Ireneçš„å…¬ä¸»å°±åœ¨é‚£è£¡å‡ºç”Ÿï¼Œä½†å¥¹å‡ºç”Ÿå¾Œä¸ä¹…ï¼Œå› ç‚ºæ¯è¦ªèº«é«”ä¸å¤ªå¥½ï¼Œä¾¿è¢«é€å»çµ¦é„‰ä¸‹äººæ’«é¤Šï¼Œä½åœ¨ä¸€æ£Ÿå¤§æˆ¿å­è£¡ï¼Œé‚£æˆ¿å­åŠæ˜¯åŸå ¡ï¼ŒåŠæ˜¯è¾²èŠï¼Œä½æ–¼å¦ä¸€åº§å±±çš„å±±è…°ä¸Šï¼Œå¤§ç´„åœ¨å±±è…³èˆ‡å±±é ‚ä¹‹é–“çš„ä¸­é»ã€‚\\n\\nå…¬ä¸»æ˜¯å€‹å¯æ„›çš„å°äººå…’ï¼Œåœ¨æˆ‘çš„æ•…äº‹é–‹å§‹æ™‚ï¼Œæˆ‘æƒ³ï¼Œå¥¹å¤§ç´„å…«æ­²ï¼Œä½†å¥¹é•·å¾—å¾ˆå¿«ã€‚å¥¹çš„è‡‰é¾ç™½çš™è€Œç¾éº—ï¼Œæœ‰è‘—ä¸€é›™åƒå…©ç‰‡å¤œç©ºçš„çœ¼ç›ï¼Œæ¯ä¸€ç‰‡æ¹›è—ä¸­éƒ½æº¶è‘—ä¸€é¡†æ˜Ÿæ˜Ÿã€‚ä½ æœƒä»¥ç‚ºé‚£é›™çœ¼ç›ä¸€å®šçŸ¥é“è‡ªå·±æ˜¯å¾é‚£è£¡ä¾†çš„ï¼Œå› ç‚ºå®ƒå€‘ç¸½æ˜¯é‚£éº¼é »ç¹åœ°æœé‚£å€‹æ–¹å‘ä»°æœ›ã€‚å¥¹è‚²å…’å®¤çš„å¤©èŠ±æ¿æ˜¯è—è‰²çš„ï¼Œä¸Šé¢æœ‰æ˜Ÿæ˜Ÿï¼Œä»–å€‘ç›¡å…¶æ‰€èƒ½åœ°å°‡å®ƒåšå¾—åƒå¤©ç©ºä¸€æ¨£ã€‚ä½†æˆ‘æ‡·ç–‘å¥¹æ˜¯å¦æ›¾è¦‹éçœŸæ­£æœ‰æ˜Ÿæ˜Ÿçš„å¤©ç©ºï¼ŒåŸå› æˆ‘æœ€å¥½ç¾åœ¨å°±èªªæ˜ç™½ã€‚',\n",
       " 'final_model': 'gemini-2.5-pro'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdba103-483d-4ccc-82a2-eaede9bb6592",
   "metadata": {},
   "source": [
    "#### Pass-fail counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e59c692-dbd6-4e5e-88de-d1375f65d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: 12, Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Only include entries where 'passed' exists\n",
    "counts = Counter(c.get('passed') for c in chunks if 'passed' in c)\n",
    "print(f\"Passed: {counts[True]}, Failed: {counts[False]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83219ed-977e-4a26-a56a-517493870ecb",
   "metadata": {},
   "source": [
    "#### Print failed entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1734a437-c7b6-4b94-8c4f-73c4e29527a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 1) Get only the entries where passed is False\n",
    "fails = [chunk for chunk in chunks if chunk.get('passed') is False]\n",
    "\n",
    "# 2) Dump them as nicely formatted JSON\n",
    "print(json.dumps(fails, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec67f8-4d95-4c62-9db7-3b5d11b89288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2b0101b-9222-4ea8-b2a4-e1693be9cd03",
   "metadata": {},
   "source": [
    "### Token size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1c5ab46-e691-490f-9066-3d72c2f637cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract token counts\n",
    "tokens = [chunk['tokens'] for chunk in chunks]\n",
    "\n",
    "# Create histogram\n",
    "plt.figure()\n",
    "plt.hist(tokens, bins='auto')\n",
    "plt.title('Distribution of Chunk Token Counts')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Number of Chunks')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a471b4-b408-429d-a94a-21e0f48b4c13",
   "metadata": {},
   "source": [
    "# Clean up failed entries, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71f69aa4-6ec8-4a3d-a7bd-695c358e54ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Re-evaluating 0 initially failed chunksâ€¦\n",
      "\n",
      "â±ï¸ Total re-evaluation (with fallback) time: 0h 0m 0.00s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mini evaluator function (reuse existing prompt logic)\n",
    "def mini_evaluate(text_to_check: str, source_text: str) -> dict:\n",
    "    evaluation_prompt = f\"\"\"\n",
    "You are a professional translation evaluator reviewing translated Biblical education materials for a Protestant/Evangelical audience.\n",
    "Your task is to assess whether a translated markdown chunk faithfully and accurately follows the original, based on these criteria:\n",
    "\n",
    "1. **Accuracy**: Meaning is preservedâ€”no additions, omissions, or summarizations.\n",
    "2. **Theological Fidelity**: Key theological terms are rendered precisely.\n",
    "3. **Markdown Integrity**: All headings, lists, blockquotes, punctuation, verse references must be preserved.\n",
    "4. **Audience Awareness**: Language reflects Protestant/Evangelical style and idioms.\n",
    "5. **Proper Names**: Preserve proper names exactly, except standard localized equivalents (e.g., Spanish Bible book titles).\n",
    "\n",
    "Source:\n",
    "{source_text}\n",
    "\n",
    "Translation:\n",
    "{text_to_check}\n",
    "\n",
    "Respond with ONLY raw JSON:\n",
    "{{\n",
    "  \"passed\": true or false,\n",
    "  \"feedback\": \"If failed, brief specific reason; empty if passed\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=openai_model,\n",
    "        messages=[{\"role\": \"user\", \"content\": evaluation_prompt}]\n",
    "    ).choices[0].message.content.strip()\n",
    "\n",
    "    clean = re.sub(r'^```(?:json)?\\r?\\n', '', resp)\n",
    "    clean = re.sub(r'\\r?\\n```$', '', clean)\n",
    "    return json.loads(clean)\n",
    "\n",
    "# Identify currently failing chunks\n",
    "fails = [chunk for chunk in chunks if not chunk.get('passed')]\n",
    "print(f\"ğŸ”„ Re-evaluating {len(fails)} initially failed chunksâ€¦\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "for idx, chunk in enumerate(fails, start=1):\n",
    "    print(f\"â†’ [{idx}/{len(fails)}] Re-checking chunk_id {chunk['chunk_id']}â€¦\")\n",
    "    verdict = mini_evaluate(chunk['check'], chunk['text'])\n",
    "    if verdict.get('passed'):\n",
    "        chunk['final'] = chunk['check']\n",
    "        chunk['final_model'] = gemini_model\n",
    "        print(f\"   âœ… chunk_id {chunk['chunk_id']} now passes primary re-check.\")\n",
    "    else:\n",
    "        print(f\"   âŒ chunk_id {chunk['chunk_id']} failed primary re-check: {verdict.get('feedback')}\\n   â†’ Attempting Claude fallbackâ€¦\")\n",
    "        # run Claude fallback\n",
    "        claude_resp = claude.messages.create(\n",
    "            model=claude_model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\",   \"content\": system_message + \"\\n\\n\" + chunk[\"text\"]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        alt_text = claude_resp.completion.strip()\n",
    "        chunk['check_alt'] = alt_text\n",
    "        # re-evaluate fallback\n",
    "        fallback_verdict = mini_evaluate(alt_text, chunk['text'])\n",
    "        if fallback_verdict.get('passed'):\n",
    "            chunk['final'] = alt_text\n",
    "            chunk['final_model'] = claude_model\n",
    "            print(f\"   âœ… chunk_id {chunk['chunk_id']} now passes with Claude fallback.\")\n",
    "        else:\n",
    "            print(f\"   âŒ chunk_id {chunk['chunk_id']} still fails fallback: {fallback_verdict.get('feedback')}\")\n",
    "\n",
    "end = time.perf_counter()\n",
    "elapsed = end - start\n",
    "hours = int(elapsed // 3600)\n",
    "minutes = int((elapsed % 3600) // 60)\n",
    "seconds = elapsed % 60\n",
    "print(f\"\\nâ±ï¸ Total re-evaluation (with fallback) time: {hours}h {minutes}m {seconds:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "859a10a8-a688-4b13-a1b6-cf2d814b423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8bf77-4678-4361-9ddd-92d41177bee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6581f8eb-831b-452e-8743-19c053a609a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'gemini-2.5-pro': 12})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# extract all final_model values (will include None if missing)\n",
    "models = [chunk.get('final_model') for chunk in chunks]\n",
    "\n",
    "# tabulate counts\n",
    "counts = Counter(models)\n",
    "\n",
    "print(counts)\n",
    "# e.g. Counter({'model_A': 123, 'model_B': 77, None: 5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedd04f-5c06-4d2e-8c2d-91e0088285f1",
   "metadata": {},
   "source": [
    "### Save json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a028279-fdbe-430f-bcbf-d855880fe5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported results to George_MacDonald_The_Princess_and_the_Goblin_50_lines_TraditionalChinese_gemini-2.5-pro_o3-2025-04-16_2025-06-27-193324.json\n"
     ]
    }
   ],
   "source": [
    "# drop the .md extension\n",
    "base = os.path.splitext(filename)[0]\n",
    "\n",
    "# remove spaces if any from output language\n",
    "output_language = translated_language.replace(\" \", \"\")\n",
    "\n",
    "# timestamp in yyyy-mm-dd-hhMMSS format\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "\n",
    "# build the output filename\n",
    "out_fname = f\"{base}_{output_language}_{gemini_model}_{openai_model}_{ts}.json\"\n",
    "\n",
    "# write to disk\n",
    "with open(out_fname, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Exported results to {out_fname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ea7cb-417d-4859-81b1-cd47bb53d4b5",
   "metadata": {},
   "source": [
    "### Create and save an interlinear markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca0db2ff-1e4c-43c3-8843-e8da5630bed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported merged markdown to George_MacDonald_The_Princess_and_the_Goblin_50_lines_TraditionalChinese_gemini-2.5-pro_o3-2025-04-16_2025-06-27-193324_interlinear.md\n"
     ]
    }
   ],
   "source": [
    "# 1) Turn your existing JSON filename into a .md filename\n",
    "base = os.path.splitext(out_fname)[0]\n",
    "markdown_fname = base + '_interlinear' + '.md'\n",
    "\n",
    "# 2) Write each chunkâ€™s original + final translation into the .md\n",
    "with open(markdown_fname, \"w\", encoding=\"utf-8\") as md_file:\n",
    "    for chunk in chunks:\n",
    "        md_file.write(chunk['text'])\n",
    "        md_file.write(\"\\n\\n\")\n",
    "        md_file.write(chunk.get('final', ''))\n",
    "        md_file.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"âœ… Exported merged markdown to {markdown_fname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5f767-2ca5-47c1-a62e-f6a4434475ae",
   "metadata": {},
   "source": [
    "### Create and save translation only markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1b84509-4139-44dc-ae0e-8d0c5021a9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported merged markdown to George_MacDonald_The_Princess_and_the_Goblin_50_lines_TraditionalChinese_gemini-2.5-pro_o3-2025-04-16_2025-06-27-193324_translation_only.md\n"
     ]
    }
   ],
   "source": [
    "# 1) Turn your existing JSON filename into a .md filename\n",
    "base = os.path.splitext(out_fname)[0]\n",
    "markdown_fname = base + '_translation_only' + '.md'\n",
    "\n",
    "# 2) Write each chunkâ€™s original + final translation into the .md\n",
    "with open(markdown_fname, \"w\", encoding=\"utf-8\") as md_file:\n",
    "    for chunk in chunks:\n",
    "        md_file.write(chunk.get('final', ''))\n",
    "        md_file.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"âœ… Exported merged markdown to {markdown_fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7dc26f-a4bd-4ea6-9063-90a62b898043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
